{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 20:13:32,514\tINFO worker.py:1538 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "import secretflow as sf\n",
    "\n",
    "# In case you have a running secretflow runtime already.\n",
    "sf.shutdown()\n",
    "# 基于这四个party，我们将会建立3个设备。\n",
    "# 一个基于 alice 的PYU设备\n",
    "# 一个基于 dave 的PYU设备\n",
    "# 一个基于 alice , bob 和 carol 的SPU设备\n",
    "sf.init(['alice', 'bob', 'carol', 'dave'], address='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nodes': [{'party': 'alice', 'address': '127.0.0.1:51451'},\n",
       "  {'party': 'bob', 'address': '127.0.0.1:38011'},\n",
       "  {'party': 'carol', 'address': '127.0.0.1:42141'}],\n",
       " 'runtime_config': {'protocol': 3, 'field': 3}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sf.utils.testing.cluster_def 是一个helper通过寻找未占用的端口来创建一个设置。\n",
    "aby3_config = sf.utils.testing.cluster_def(parties=['alice', 'bob', 'carol'])\n",
    "\n",
    "aby3_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nodes': [{'party': 'alice', 'address': '127.0.0.1:51451'},\n",
       "  {'party': 'bob', 'address': '127.0.0.1:38011'},\n",
       "  {'party': 'carol', 'address': '127.0.0.1:42141'}],\n",
       " 'runtime_config': {'protocol': 3, 'field': 3}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 随后我们用 aby3_config 来创建一个SPU设备并检查其 cluster_def 。\n",
    "spu_device = sf.SPU(aby3_config)\n",
    "\n",
    "spu_device.cluster_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 们创建两个PYU设备。\n",
    "alice, dave = sf.PYU('alice'), sf.PYU('dave')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<secretflow.device.device.pyu.PYUObject at 0x7f0b7077d910>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 首先，我们用一个PYU设备创建一个PYU object。\n",
    "def debit_amount():\n",
    "    return 10\n",
    "\n",
    "\n",
    "debit_amount_pyu = alice(debit_amount)()\n",
    "debit_amount_pyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<secretflow.device.device.spu.SPUObject at 0x7f0b72824d30>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_run pid=24900)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=24900)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "\u001b[2m\u001b[36m(_run pid=24900)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "\u001b[2m\u001b[36m(_run pid=24900)\u001b[0m INFO:jax._src.xla_bridge:Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "\u001b[2m\u001b[36m(_run pid=24900)\u001b[0m WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# 然后我们将debit_amount_pyu从PYU传到SPU，我们将会得到一个SPU object作为结果。\n",
    "debit_amount_spu = debit_amount_pyu.to(spu_device)\n",
    "\n",
    "debit_amount_spu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', 'data', 'device', 'device_type', 'to']\n",
      "ObjectRef(32d950ec0ccf9d2affffffffffffffffffffffff0100000001000000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-06-01 20:30:32,441 E 24794 24794] (raylet) node_manager.cc:3097: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 019f9c5ff5f0db414ccc6a7f58aeed1f0edf9097082fb2918e01d5b9, IP: 10.136.75.128) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.136.75.128`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-06-01 20:52:32,475 E 24794 24794] (raylet) node_manager.cc:3097: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 019f9c5ff5f0db414ccc6a7f58aeed1f0edf9097082fb2918e01d5b9, IP: 10.136.75.128) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.136.75.128`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-06-01 20:53:32,476 E 24794 24794] (raylet) node_manager.cc:3097: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 019f9c5ff5f0db414ccc6a7f58aeed1f0edf9097082fb2918e01d5b9, IP: 10.136.75.128) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.136.75.128`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "# print(debit_amount_pyu.meta)\n",
    "# pyu没有meta和sharenames好像，上面那个玩意会报错\n",
    "print(dir(debit_amount_pyu))\n",
    "print(debit_amount_pyu.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectRef(e0dc174c83599034ffffffffffffffffffffffff0100000001000000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 它是一个在alice一边的Ray ObjectRef\n",
    "debit_amount_spu.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ObjectRef(f4402ec78d3a26076093ec3f85f7046ff7565a3d0100000001000000),\n",
       " ObjectRef(f91b78d7db9a65937bf1dc25d2421469a0f9c22b0100000001000000),\n",
       " ObjectRef(82891771158d68c152418d0ea3ad27761416c8f80100000001000000)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 会得到一个ObjectRef列表。因为它在alice这一侧，我们无法在host检查它的值。\n",
    "# 不太懂什么叫无法在host检查他的值\n",
    "debit_amount_spu.shares_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(10, dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-06-01 20:20:32,426 E 24794 24794] (raylet) node_manager.cc:3097: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 019f9c5ff5f0db414ccc6a7f58aeed1f0edf9097082fb2918e01d5b9, IP: 10.136.75.128) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.136.75.128`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "sf.reveal(debit_amount_spu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('sf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54d06d589defdc7aa03c59811d58e9f2406cda7d5d8b6be0920ef29f568be004"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
